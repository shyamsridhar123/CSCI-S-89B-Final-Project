{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8e9ae58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MODEL SETUP AND DOWNLOAD\n",
      "============================================================\n",
      "\n",
      "Python version: 3.11.13 (main, Jun  5 2025, 13:12:00) [GCC 11.2.0]\n",
      "Working directory: /home/shyamsridhar/code/NLPFinalProject/notebooks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL SETUP AND DOWNLOAD\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nPython version: {sys.version}\")\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042d1922",
   "metadata": {},
   "source": [
    "## 1. Download FinBERT (Financial Sentiment Analysis)\n",
    "\n",
    "FinBERT is a BERT model fine-tuned on financial text. It's used for sentiment analysis of SEC filings.\n",
    "\n",
    "- **Model**: `ProsusAI/finbert`\n",
    "- **Size**: ~500MB\n",
    "- **Cache Location**: `~/.cache/huggingface/hub/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83b32b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/3] Downloading FinBERT model: ProsusAI/finbert\n",
      "      This may take 2-5 minutes on first run...\n",
      "\n",
      "  â†’ Downloading tokenizer...\n",
      "    âœ… Tokenizer cached\n",
      "  â†’ Downloading model weights...\n",
      "    âœ… Model cached\n",
      "\n",
      "  Model config:\n",
      "    - Architecture: ['BertForSequenceClassification']\n",
      "    - Labels: {0: 'positive', 1: 'negative', 2: 'neutral'}\n",
      "    - Hidden size: 768\n",
      "\n",
      "  Cache location: /home/shyamsridhar/.cache/huggingface/hub/models--ProsusAI--finbert\n",
      "  Cache size: 1671.2 MB\n",
      "\n",
      "âœ… FinBERT download complete!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "MODEL_NAME = \"ProsusAI/finbert\"\n",
    "\n",
    "print(f\"\\n[1/3] Downloading FinBERT model: {MODEL_NAME}\")\n",
    "print(\"      This may take 2-5 minutes on first run...\\n\")\n",
    "\n",
    "# Download tokenizer\n",
    "print(\"  â†’ Downloading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "print(\"    âœ… Tokenizer cached\")\n",
    "\n",
    "# Download model\n",
    "print(\"  â†’ Downloading model weights...\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "print(\"    âœ… Model cached\")\n",
    "\n",
    "# Verify\n",
    "print(f\"\\n  Model config:\")\n",
    "print(f\"    - Architecture: {model.config.architectures}\")\n",
    "print(f\"    - Labels: {model.config.id2label}\")\n",
    "print(f\"    - Hidden size: {model.config.hidden_size}\")\n",
    "\n",
    "# Check cache location\n",
    "cache_dir = os.path.expanduser(\"~/.cache/huggingface/hub\")\n",
    "finbert_cache = os.path.join(cache_dir, \"models--ProsusAI--finbert\")\n",
    "if os.path.exists(finbert_cache):\n",
    "    size_mb = sum(os.path.getsize(os.path.join(dp, f)) for dp, dn, fn in os.walk(finbert_cache) for f in fn) / (1024*1024)\n",
    "    print(f\"\\n  Cache location: {finbert_cache}\")\n",
    "    print(f\"  Cache size: {size_mb:.1f} MB\")\n",
    "\n",
    "print(\"\\nâœ… FinBERT download complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8698ef29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing FinBERT sentiment analysis...\n",
      "\n",
      "  Text: \"Revenue increased 15% year-over-year, exceeding analyst expe...\"\n",
      "  â†’ positive: 95.75%\n",
      "\n",
      "  Text: \"The company reported a significant loss due to supply chain ...\"\n",
      "  â†’ negative: 96.90%\n",
      "\n",
      "  Text: \"The quarterly dividend was declared at $0.50 per share....\"\n",
      "  â†’ neutral: 78.91%\n",
      "\n",
      "âœ… FinBERT is working correctly!\n"
     ]
    }
   ],
   "source": [
    "# Quick test of FinBERT\n",
    "from transformers import pipeline\n",
    "\n",
    "print(\"Testing FinBERT sentiment analysis...\\n\")\n",
    "\n",
    "sentiment_pipeline = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    truncation=True,\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "test_texts = [\n",
    "    \"Revenue increased 15% year-over-year, exceeding analyst expectations.\",\n",
    "    \"The company reported a significant loss due to supply chain disruptions.\",\n",
    "    \"The quarterly dividend was declared at $0.50 per share.\"\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    result = sentiment_pipeline(text)[0]\n",
    "    print(f\"  Text: \\\"{text[:60]}...\\\"\")\n",
    "    print(f\"  â†’ {result['label']}: {result['score']:.2%}\\n\")\n",
    "\n",
    "print(\"âœ… FinBERT is working correctly!\")\n",
    "\n",
    "# Clean up memory\n",
    "del model, tokenizer, sentiment_pipeline\n",
    "import gc\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8be2d8",
   "metadata": {},
   "source": [
    "## 2. Download spaCy Model (Named Entity Recognition)\n",
    "\n",
    "spaCy's `en_core_web_sm` model is used for extracting named entities like organizations, people, dates, and monetary values.\n",
    "\n",
    "- **Model**: `en_core_web_sm`\n",
    "- **Size**: ~12MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "195718e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2/3] Setting up spaCy model: en_core_web_sm\n",
      "  âœ… Model already installed\n",
      "\n",
      "  Model info:\n",
      "    - Pipeline: ['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "    - Vectors: (0, 0)\n",
      "\n",
      "âœ… spaCy setup complete!\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import spacy\n",
    "\n",
    "print(\"\\n[2/3] Setting up spaCy model: en_core_web_sm\")\n",
    "\n",
    "# Check if already installed\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    print(\"  âœ… Model already installed\")\n",
    "except OSError:\n",
    "    print(\"  â†’ Downloading model...\")\n",
    "    subprocess.run([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"], \n",
    "                   capture_output=True)\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    print(\"  âœ… Model downloaded and installed\")\n",
    "\n",
    "# Verify\n",
    "print(f\"\\n  Model info:\")\n",
    "print(f\"    - Pipeline: {nlp.pipe_names}\")\n",
    "print(f\"    - Vectors: {nlp.vocab.vectors.shape}\")\n",
    "\n",
    "print(\"\\nâœ… spaCy setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0493e86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing spaCy named entity recognition...\n",
      "\n",
      "  Text: \"Apple Inc. reported revenue of $94.8 billion for Q1 2024. CEO Tim Cook announced expansion plans in Europe.\"\n",
      "\n",
      "  Entities found:\n",
      "    - Apple Inc. (ORG)\n",
      "    - $94.8 billion (MONEY)\n",
      "    - Q1 2024 (DATE)\n",
      "    - Tim Cook (PERSON)\n",
      "    - Europe (LOC)\n",
      "\n",
      "âœ… spaCy NER is working correctly!\n"
     ]
    }
   ],
   "source": [
    "# Quick test of spaCy NER\n",
    "print(\"Testing spaCy named entity recognition...\\n\")\n",
    "\n",
    "test_text = \"Apple Inc. reported revenue of $94.8 billion for Q1 2024. CEO Tim Cook announced expansion plans in Europe.\"\n",
    "\n",
    "doc = nlp(test_text)\n",
    "\n",
    "print(f\"  Text: \\\"{test_text}\\\"\\n\")\n",
    "print(\"  Entities found:\")\n",
    "for ent in doc.ents:\n",
    "    print(f\"    - {ent.text} ({ent.label_})\")\n",
    "\n",
    "print(\"\\nâœ… spaCy NER is working correctly!\")\n",
    "\n",
    "del nlp, doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b984777e",
   "metadata": {},
   "source": [
    "## 3. Download NLTK Data (Sentence Tokenization)\n",
    "\n",
    "NLTK's `punkt` tokenizer is used for splitting text into sentences (used by the Forward-Looking Statement Detector).\n",
    "\n",
    "- **Data**: `punkt`, `punkt_tab`\n",
    "- **Size**: ~2MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63ae0891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[3/3] Downloading NLTK data\n",
      "  â†’ Downloading punkt tokenizer...\n",
      "  âœ… NLTK data downloaded\n",
      "\n",
      "  NLTK data path: /home/shyamsridhar/nltk_data\n",
      "\n",
      "âœ… NLTK setup complete!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "print(\"\\n[3/3] Downloading NLTK data\")\n",
    "\n",
    "# Download punkt tokenizer\n",
    "print(\"  â†’ Downloading punkt tokenizer...\")\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "print(\"  âœ… NLTK data downloaded\")\n",
    "\n",
    "# Verify\n",
    "print(f\"\\n  NLTK data path: {nltk.data.path[0]}\")\n",
    "\n",
    "print(\"\\nâœ… NLTK setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb6ae923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing NLTK sentence tokenization...\n",
      "\n",
      "  Input text: \"The company expects revenue to grow 10% next year. We believe this will be driven by new product launches. However, market conditions may affect these projections.\"\n",
      "\n",
      "  Sentences found: 3\n",
      "    1. The company expects revenue to grow 10% next year.\n",
      "    2. We believe this will be driven by new product launches.\n",
      "    3. However, market conditions may affect these projections.\n",
      "\n",
      "âœ… NLTK tokenization is working correctly!\n"
     ]
    }
   ],
   "source": [
    "# Quick test of NLTK\n",
    "print(\"Testing NLTK sentence tokenization...\\n\")\n",
    "\n",
    "test_text = \"The company expects revenue to grow 10% next year. We believe this will be driven by new product launches. However, market conditions may affect these projections.\"\n",
    "\n",
    "sentences = nltk.sent_tokenize(test_text)\n",
    "\n",
    "print(f\"  Input text: \\\"{test_text}\\\"\\n\")\n",
    "print(f\"  Sentences found: {len(sentences)}\")\n",
    "for i, sent in enumerate(sentences, 1):\n",
    "    print(f\"    {i}. {sent}\")\n",
    "\n",
    "print(\"\\nâœ… NLTK tokenization is working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a1d4bc",
   "metadata": {},
   "source": [
    "## 4. Verify TensorFlow/Keras (Document Classifier)\n",
    "\n",
    "The document classifier uses TensorFlow/Keras. No download needed, but let's verify the installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8716a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Bonus] Verifying TensorFlow/Keras installation\n",
      "  TensorFlow version: 2.20.0\n",
      "  Keras version: 3.11.3\n",
      "  GPU available: /physical_device:GPU:0\n",
      "\n",
      "âœ… TensorFlow/Keras ready!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "print(\"\\n[Bonus] Verifying TensorFlow/Keras installation\")\n",
    "print(f\"  TensorFlow version: {tf.__version__}\")\n",
    "print(f\"  Keras version: {keras.__version__}\")\n",
    "\n",
    "# Check GPU\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"  GPU available: {gpus[0].name}\")\n",
    "else:\n",
    "    print(\"  GPU: Not available (CPU mode - this is fine)\")\n",
    "\n",
    "print(\"\\nâœ… TensorFlow/Keras ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da1e3f2",
   "metadata": {},
   "source": [
    "## 5. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0145f5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "MODEL SETUP COMPLETE\n",
      "============================================================\n",
      "\n",
      "âœ… All models downloaded and cached:\n",
      "\n",
      "  ðŸ“¦ FinBERT (ProsusAI/finbert)\n",
      "     Size: 1671 MB\n",
      "     Location: ~/.cache/huggingface/hub/\n",
      "\n",
      "  ðŸ“¦ spaCy (en_core_web_sm)\n",
      "     Size: ~12 MB\n",
      "     Location: Python site-packages\n",
      "\n",
      "  ðŸ“¦ NLTK (punkt)\n",
      "     Size: ~2 MB\n",
      "     Location: /home/shyamsridhar/nltk_data\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "NEXT STEPS:\n",
      "------------------------------------------------------------\n",
      "\n",
      "1. Run 01_data_preparation.ipynb to download training data\n",
      "2. Run 02_train_classifier.ipynb to train the document classifier\n",
      "3. Run 'python app.py' to launch the dashboard\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL SETUP COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nâœ… All models downloaded and cached:\\n\")\n",
    "\n",
    "# Check cache sizes\n",
    "cache_info = []\n",
    "\n",
    "# FinBERT\n",
    "finbert_path = os.path.expanduser(\"~/.cache/huggingface/hub/models--ProsusAI--finbert\")\n",
    "if os.path.exists(finbert_path):\n",
    "    size = sum(os.path.getsize(os.path.join(dp, f)) for dp, dn, fn in os.walk(finbert_path) for f in fn) / (1024*1024)\n",
    "    cache_info.append((\"FinBERT (ProsusAI/finbert)\", f\"{size:.0f} MB\", \"~/.cache/huggingface/hub/\"))\n",
    "\n",
    "# spaCy\n",
    "try:\n",
    "    import spacy\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    cache_info.append((\"spaCy (en_core_web_sm)\", \"~12 MB\", \"Python site-packages\"))\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# NLTK\n",
    "nltk_path = os.path.expanduser(\"~/nltk_data\")\n",
    "if os.path.exists(nltk_path):\n",
    "    cache_info.append((\"NLTK (punkt)\", \"~2 MB\", nltk_path))\n",
    "\n",
    "for name, size, location in cache_info:\n",
    "    print(f\"  ðŸ“¦ {name}\")\n",
    "    print(f\"     Size: {size}\")\n",
    "    print(f\"     Location: {location}\\n\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"NEXT STEPS:\")\n",
    "print(\"-\"*60)\n",
    "print(\"\\n1. Run 01_data_preparation.ipynb to download training data\")\n",
    "print(\"2. Run 02_train_classifier.ipynb to train the document classifier\")\n",
    "print(\"3. Run 'python app.py' to launch the dashboard\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-ml-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
