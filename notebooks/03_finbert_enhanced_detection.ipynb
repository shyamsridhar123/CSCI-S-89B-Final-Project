{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b624efff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.6.0+cu124\n",
      "CUDA: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import time\n",
    "import warnings\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "import torch\n",
    "import nltk\n",
    "from transformers import pipeline\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "PROJECT_ROOT = os.path.dirname(os.getcwd())\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "for resource in ['tokenizers/punkt', 'tokenizers/punkt_tab']:\n",
    "    try:\n",
    "        nltk.data.find(resource)\n",
    "    except LookupError:\n",
    "        nltk.download(resource.split('/')[-1], quiet=True)\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377f8204",
   "metadata": {},
   "source": [
    "## 1. Load FinBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d053c4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FinBERT loaded on cuda:0\n"
     ]
    }
   ],
   "source": [
    "finbert = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"ProsusAI/finbert\",\n",
    "    framework=\"pt\",\n",
    "    truncation=True,\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "print(f\"FinBERT loaded on {finbert.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65876e31",
   "metadata": {},
   "source": [
    "## 2. Forward-Looking Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f4fca5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForwardLookingDetector:\n",
    "    \"\"\"Detects forward-looking statements using keywords + FinBERT sentiment.\"\"\"\n",
    "    \n",
    "    PATTERNS = {\n",
    "        'high': [r'\\bwill\\b', r'\\bexpect', r'\\bplan\\s+to', r'\\bintend', r'\\bcommitted\\s+to'],\n",
    "        'medium': [r'\\bbelieve', r'\\banticipate', r'\\bproject', r'\\bforecast', r'\\bestimate'],\n",
    "        'low': [r'\\bmay\\b', r'\\bcould\\b', r'\\bmight\\b', r'\\bpossible', r'\\bwould\\b']\n",
    "    }\n",
    "    \n",
    "    WEIGHTS = {'high': 0.9, 'medium': 0.7, 'low': 0.5}\n",
    "    \n",
    "    def __init__(self, sentiment_pipeline):\n",
    "        self.sentiment = sentiment_pipeline\n",
    "        self._compiled = {\n",
    "            level: [re.compile(p, re.IGNORECASE) for p in patterns]\n",
    "            for level, patterns in self.PATTERNS.items()\n",
    "        }\n",
    "    \n",
    "    def _classify_keyword(self, text: str) -> Tuple[Optional[str], Optional[str]]:\n",
    "        for level in ['high', 'medium', 'low']:\n",
    "            for pattern in self._compiled[level]:\n",
    "                match = pattern.search(text)\n",
    "                if match:\n",
    "                    return level, match.group()\n",
    "        return None, None\n",
    "    \n",
    "    def detect(self, text: str) -> List[Dict]:\n",
    "        results = []\n",
    "        for sent in nltk.sent_tokenize(text):\n",
    "            level, keyword = self._classify_keyword(sent)\n",
    "            if level:\n",
    "                result = self.sentiment(sent[:512])[0]\n",
    "                sentiment = result['label'].lower()\n",
    "                score = result['score']\n",
    "                modifier = {'positive': 1.0, 'neutral': 0.9, 'negative': 0.8}[sentiment]\n",
    "                combined = self.WEIGHTS[level] * modifier * score\n",
    "                \n",
    "                results.append({\n",
    "                    'sentence': sent,\n",
    "                    'keyword_level': level,\n",
    "                    'keyword': keyword,\n",
    "                    'sentiment': sentiment,\n",
    "                    'sentiment_score': score,\n",
    "                    'combined_score': combined\n",
    "                })\n",
    "        \n",
    "        return sorted(results, key=lambda x: -x['combined_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343a6c52",
   "metadata": {},
   "source": [
    "## 3. Risk Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ab0cd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RiskDetector:\n",
    "    \"\"\"Detects risk factors using patterns + FinBERT for severity.\"\"\"\n",
    "    \n",
    "    PATTERNS = {\n",
    "        'regulatory': [r'regulatory\\s+risk', r'litigation', r'antitrust', r'compliance'],\n",
    "        'financial': [r'material\\s+adverse', r'liquidity', r'impairment', r'going\\s+concern'],\n",
    "        'operational': [r'supply\\s+chain', r'cybersecurity', r'data\\s+breach', r'system\\s+failure'],\n",
    "        'market': [r'competition', r'market\\s+volatility', r'economic\\s+downturn', r'currency'],\n",
    "        'geopolitical': [r'geopolitical', r'trade\\s+war', r'tariff', r'sanction'],\n",
    "        'climate': [r'climate', r'environmental', r'ESG', r'sustainability']\n",
    "    }\n",
    "    \n",
    "    def __init__(self, sentiment_pipeline):\n",
    "        self.sentiment = sentiment_pipeline\n",
    "        self._compiled = {\n",
    "            cat: [re.compile(p, re.IGNORECASE) for p in patterns]\n",
    "            for cat, patterns in self.PATTERNS.items()\n",
    "        }\n",
    "    \n",
    "    def _get_sentence(self, text: str, start: int) -> str:\n",
    "        for sent in nltk.sent_tokenize(text):\n",
    "            pos = text.find(sent)\n",
    "            if pos <= start < pos + len(sent):\n",
    "                return sent\n",
    "        return text[max(0, start-50):start+100]\n",
    "    \n",
    "    def detect(self, text: str) -> Dict[str, List[Dict]]:\n",
    "        results = {}\n",
    "        for category, patterns in self._compiled.items():\n",
    "            risks = []\n",
    "            seen = set()\n",
    "            for pattern in patterns:\n",
    "                for match in pattern.finditer(text):\n",
    "                    context = self._get_sentence(text, match.start())\n",
    "                    if context in seen:\n",
    "                        continue\n",
    "                    seen.add(context)\n",
    "                    \n",
    "                    result = self.sentiment(context[:512])[0]\n",
    "                    sentiment = result['label'].lower()\n",
    "                    score = result['score']\n",
    "                    \n",
    "                    if sentiment == 'negative':\n",
    "                        severity, sev_score = 'high', 0.7 + 0.3 * score\n",
    "                    elif sentiment == 'neutral':\n",
    "                        severity, sev_score = 'medium', 0.4 + 0.3 * score\n",
    "                    else:\n",
    "                        severity, sev_score = 'low', 0.1 + 0.2 * score\n",
    "                    \n",
    "                    risks.append({\n",
    "                        'match': match.group(),\n",
    "                        'context': context,\n",
    "                        'severity': severity,\n",
    "                        'severity_score': sev_score,\n",
    "                        'sentiment': sentiment\n",
    "                    })\n",
    "            if risks:\n",
    "                results[category] = risks\n",
    "        return results\n",
    "    \n",
    "    def score(self, risks: Dict) -> Tuple[float, str]:\n",
    "        if not risks:\n",
    "            return 0.0, 'Low'\n",
    "        scores = [r['severity_score'] for cat in risks.values() for r in cat]\n",
    "        avg = sum(scores) / len(scores) * 100\n",
    "        qty = min(1.0, len(scores) / 20)\n",
    "        final = min(100, avg * 0.7 + qty * 100 * 0.3)\n",
    "        level = 'High' if final >= 60 else 'Medium' if final >= 30 else 'Low'\n",
    "        return final, level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3f9e89",
   "metadata": {},
   "source": [
    "## 4. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78528d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_FL = \"\"\"\n",
    "Apple reported revenue of $94.8 billion for Q4 2024.\n",
    "We expect revenue to grow 15% next fiscal year.\n",
    "The company believes services will continue to drive growth.\n",
    "Management anticipates margin improvement.\n",
    "There may be currency headwinds.\n",
    "We are committed to returning capital to shareholders.\n",
    "Market conditions could adversely affect business.\n",
    "\"\"\"\n",
    "\n",
    "TEST_RISK = \"\"\"\n",
    "The Company faces significant regulatory risk.\n",
    "We may experience material adverse effects from supply chain disruptions.\n",
    "Geopolitical tensions could impact operations.\n",
    "Competition is intense with pricing pressure.\n",
    "Cybersecurity threats pose operational risk.\n",
    "Climate regulations may require capital investments.\n",
    "Our diversified model mitigates many risks.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3e44d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward-Looking Statements\n",
      "============================================================\n",
      "[HIGH] 'expect' | positive (0.95) | combined=0.857\n",
      "  We expect revenue to grow 15% next fiscal year.\n",
      "[HIGH] 'will' | positive (0.94) | combined=0.842\n",
      "  The company believes services will continue to drive growth.\n",
      "[MEDIUM] 'anticipate' | positive (0.95) | combined=0.668\n",
      "  Management anticipates margin improvement.\n",
      "[HIGH] 'committed to' | positive (0.58) | combined=0.523\n",
      "  We are committed to returning capital to shareholders.\n",
      "[LOW] 'may' | negative (0.94) | combined=0.375\n",
      "  There may be currency headwinds.\n",
      "[LOW] 'could' | negative (0.89) | combined=0.355\n",
      "  Market conditions could adversely affect business.\n",
      "\n",
      "Total: 6\n"
     ]
    }
   ],
   "source": [
    "fl_detector = ForwardLookingDetector(finbert)\n",
    "fl_results = fl_detector.detect(TEST_FL)\n",
    "\n",
    "print(\"Forward-Looking Statements\")\n",
    "print(\"=\" * 60)\n",
    "for r in fl_results:\n",
    "    print(f\"[{r['keyword_level'].upper()}] '{r['keyword']}' | {r['sentiment']} ({r['sentiment_score']:.2f}) | combined={r['combined_score']:.3f}\")\n",
    "    print(f\"  {r['sentence'][:80]}\")\n",
    "print(f\"\\nTotal: {len(fl_results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2270e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Risk Detection\n",
      "============================================================\n",
      "Score: 67/100 (High)\n",
      "\n",
      "regulatory: 1 risks\n",
      "  [MEDIUM] regulatory risk | neutral\n",
      "financial: 1 risks\n",
      "  [HIGH] material adverse | negative\n",
      "operational: 2 risks\n",
      "  [HIGH] supply chain | negative\n",
      "  [HIGH] Cybersecurity | negative\n",
      "market: 1 risks\n",
      "  [MEDIUM] Competition | neutral\n",
      "geopolitical: 1 risks\n",
      "  [HIGH] Geopolitical | negative\n",
      "climate: 1 risks\n",
      "  [MEDIUM] Climate | neutral\n"
     ]
    }
   ],
   "source": [
    "risk_detector = RiskDetector(finbert)\n",
    "risk_results = risk_detector.detect(TEST_RISK)\n",
    "risk_score, risk_level = risk_detector.score(risk_results)\n",
    "\n",
    "print(\"Risk Detection\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Score: {risk_score:.0f}/100 ({risk_level})\\n\")\n",
    "for cat, risks in risk_results.items():\n",
    "    print(f\"{cat}: {len(risks)} risks\")\n",
    "    for r in risks:\n",
    "        print(f\"  [{r['severity'].upper()}] {r['match']} | {r['sentiment']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20c123e",
   "metadata": {},
   "source": [
    "## 5. Baseline Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90e5dc1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Forward-looking detector ready (keyword-based)\n",
      "  ✓ Risk detector ready (keyword-based)\n",
      "Baseline vs Enhanced\n",
      "========================================\n",
      "Forward-Looking: 6 vs 6\n",
      "Risk Score: 12 (Low) vs 67 (High)\n"
     ]
    }
   ],
   "source": [
    "from src.risk_detector import RiskDetector as BaselineRisk\n",
    "from src.metrics_extractor import ForwardLookingDetector as BaselineFL\n",
    "\n",
    "baseline_fl = BaselineFL()\n",
    "baseline_risk = BaselineRisk()\n",
    "\n",
    "b_fl = baseline_fl.detect(TEST_FL)\n",
    "b_risk = baseline_risk.detect(TEST_RISK)\n",
    "b_score, b_level = baseline_risk.get_risk_score(b_risk)\n",
    "\n",
    "print(\"Baseline vs Enhanced\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Forward-Looking: {len(b_fl)} vs {len(fl_results)}\")\n",
    "print(f\"Risk Score: {b_score:.0f} ({b_level}) vs {risk_score:.0f} ({risk_level})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eef4d8b",
   "metadata": {},
   "source": [
    "## 6. Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9db38da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test size: 5,000 chars\n",
      "Baseline: 0.013s\n",
      "Enhanced: 0.557s\n",
      "Overhead: 43.0x\n"
     ]
    }
   ],
   "source": [
    "sample_path = os.path.join(PROJECT_ROOT, 'data', 'sample_docs', 'sample_section_1A.txt')\n",
    "large_text = open(sample_path).read() if os.path.exists(sample_path) else TEST_RISK * 5\n",
    "print(f\"Test size: {len(large_text):,} chars\")\n",
    "\n",
    "t0 = time.time()\n",
    "_ = baseline_risk.detect(large_text)\n",
    "_ = baseline_fl.detect(large_text)\n",
    "t_baseline = time.time() - t0\n",
    "\n",
    "t0 = time.time()\n",
    "_ = risk_detector.detect(large_text)\n",
    "_ = fl_detector.detect(large_text)\n",
    "t_enhanced = time.time() - t0\n",
    "\n",
    "print(f\"Baseline: {t_baseline:.3f}s\")\n",
    "print(f\"Enhanced: {t_enhanced:.3f}s\")\n",
    "print(f\"Overhead: {t_enhanced/t_baseline:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb89a66",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "- FinBERT adds sentiment-based severity scoring\n",
    "- Distinguishes acknowledged vs severe risks\n",
    "- Trade-off: increased inference time\n",
    "\n",
    "**Usage**: Pass `finbert_pipeline` to `RiskDetector` or `ForwardLookingDetector` in `src/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2bc4150c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "del finbert, fl_detector, risk_detector\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "print(\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-ml-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
